{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### If you are running this on google colab, mount your google drive first\n",
    "## If not, comment out the following lines\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Importing the necessary module\n",
    "from transformed import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Importing necessary libraries\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### Create an SVM classifier with a linear kernel\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "### Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "### Predict the labels on the test data\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.a - Selecting a Loss Function\n",
    "\n",
    "### Define the loss function as mean squared error\n",
    "# def loss_function(y_true, y_pred):\n",
    "#   return mean_squared_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.b - Selecting an Optimization Algorithm\n",
    "\n",
    "# ### Define the gradient of the loss function with respect to the SVM parameters\n",
    "# def gradient_function(clf, X, y):\n",
    "#   # Get the current parameters of the SVM\n",
    "#   w = clf.coef_\n",
    "#   b = clf.intercept_\n",
    "\n",
    "#   # Get the number of samples and features\n",
    "#   n_samples, n_features = X.shape\n",
    "\n",
    "#   # Initialize the gradient vectors\n",
    "#   grad_w = np.zeros((n_features, 1))\n",
    "#   grad_b = np.zeros((1,))\n",
    "\n",
    "#   # Loop over the samples\n",
    "#   for i in range(n_samples):\n",
    "#     # Get the current sample and label\n",
    "#     x_i = X[i].reshape((1, -1))\n",
    "#     y_i = y[i]\n",
    "\n",
    "#     # Compute the margin\n",
    "#     margin = y_i * (x_i.dot(w) + b)\n",
    "\n",
    "#     # Check if the sample is correctly classified or not\n",
    "#     if margin >= 1:\n",
    "#       # No contribution to the gradient\n",
    "#       continue\n",
    "#     else:\n",
    "#       # Add the contribution to the gradient\n",
    "#       grad_w += -y_i * x_i.T\n",
    "#       grad_b += -y_i\n",
    "\n",
    "#   # Return the gradient vectors\n",
    "#   return grad_w, grad_b\n",
    "\n",
    "### Define the gradient descent algorithm\n",
    "# def gradient_descent(X_train, y_train, X_test, y_test, learning_rate=0.01, max_iter=100):\n",
    "#   # Create an SVM classifier with linear kernel\n",
    "#   clf = SVC(kernel='linear')\n",
    "\n",
    "#   # Fit the classifier on the training data\n",
    "#   clf.fit(X_train, y_train)\n",
    "\n",
    "#   # Initialize the list of losses\n",
    "#   losses = []\n",
    "\n",
    "#   # Loop over the iterations\n",
    "#   for i in range(max_iter):\n",
    "#     # Predict the labels on the test data\n",
    "#     y_pred = clf.predict(X_test)\n",
    "\n",
    "#     # Calculate the loss on the test data\n",
    "#     loss = loss_function(y_test, y_pred)\n",
    "\n",
    "#     # Append the loss to the list\n",
    "#     losses.append(loss)\n",
    "\n",
    "#     # Print the loss every 10 iterations\n",
    "#     if (i + 1) % 10 == 0:\n",
    "#       print(f'Iteration {i + 1}, Loss: {loss}')\n",
    "\n",
    "#     # Calculate the gradient of the loss function with respect to the SVM parameters\n",
    "#     grad_w, grad_b = gradient_function(clf, X_train, y_train)\n",
    "\n",
    "#     # Update the SVM parameters using the gradient descent formula\n",
    "#     clf.coef_ -= learning_rate * grad_w\n",
    "#     clf.intercept_ -= learning_rate * grad_b\n",
    "\n",
    "#   # Return the final classifier and the list of losses\n",
    "#   return clf, losses\n",
    "\n",
    "# ### Call the gradient descent function with some parameters\n",
    "# clf, losses = gradient_descent(X_train, y_train, X_test, y_test, learning_rate=0.01, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### Plot the losses over iterations\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(range(1, len(losses) + 1), losses)\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Gradient Descent for SVM')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.c - Selecting a Learning Rate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.d - Selecting a Regulization Technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.e - Selecting a Validation Strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Importing libraries - accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 3.a - Select Evaluation Metrics\n",
    "\n",
    "### For Binomial Classification - Attack vs Normal\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "## Print the evaluation metrics\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1:', f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### For Multinomial Classification - Normal vs Back vs BufferOverflow vs FTPWrite vs GuessPassword vs NMap vs Neptune vs PortSweep vs RootKit vs Satan vs Smurf\n",
    "\n",
    "### Define the list of class names for the 10 types of attacks and normal\n",
    "class_names = ['normal', 'back', 'buffer_overflow', 'ftp_write', 'guess_passwd', 'neptune', 'nmap', 'portsweep', 'rootkit', 'satan', 'smurf']\n",
    "\n",
    "### Generate the classification report from y_test and y_pred, using the class names\n",
    "report = classification_report(y_test, y_pred, target_names=class_names)\n",
    "\n",
    "### Print the report\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
