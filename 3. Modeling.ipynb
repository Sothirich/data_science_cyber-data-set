{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### If you are running this on google colab, mount your google drive first\n",
    "## If not, comment out the following lines\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Importing the necessary module\n",
    "from transformed import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Importing necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "# ### Create an SVM classifier with linear kernel\n",
    "# clf = SVC(kernel='linear')\n",
    "\n",
    "# ### Fit the classifier on the training data\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# ### Predict the labels on the test data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.a - Selecting a Loss Function\n",
    "\n",
    "### Define the loss function as mean squared error\n",
    "def loss_function(y_true, y_pred):\n",
    "  return mean_squared_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.b - Selecting an Optimization Algorithm\n",
    "\n",
    "### Define the gradient of the loss function with respect to the SVM parameters\n",
    "def gradient_function(clf, X, y):\n",
    "  # Get the current parameters of the SVM\n",
    "  w = clf.coef_\n",
    "  b = clf.intercept_\n",
    "\n",
    "  # Get the number of samples and features\n",
    "  n_samples, n_features = X.shape\n",
    "\n",
    "  # Initialize the gradient vectors\n",
    "  grad_w = np.zeros((n_features, 1))\n",
    "  grad_b = np.zeros((1,))\n",
    "\n",
    "  # Loop over the samples\n",
    "  for i in range(n_samples):\n",
    "    # Get the current sample and label\n",
    "    x_i = X[i].reshape((1, -1))\n",
    "    y_i = y[i]\n",
    "\n",
    "    # Compute the margin\n",
    "    margin = y_i * (x_i.dot(w) + b)\n",
    "\n",
    "    # Check if the sample is correctly classified or not\n",
    "    if margin >= 1:\n",
    "      # No contribution to the gradient\n",
    "      continue\n",
    "    else:\n",
    "      # Add the contribution to the gradient\n",
    "      grad_w += -y_i * x_i.T\n",
    "      grad_b += -y_i\n",
    "\n",
    "  # Return the gradient vectors\n",
    "  return grad_w, grad_b\n",
    "\n",
    "### Define the gradient descent algorithm\n",
    "def gradient_descent(X_train, y_train, X_test, y_test, learning_rate=0.01, max_iter=100):\n",
    "  # Create an SVM classifier with linear kernel\n",
    "  clf = SVC(kernel='linear')\n",
    "\n",
    "  # Fit the classifier on the training data\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "  # Initialize the list of losses\n",
    "  losses = []\n",
    "\n",
    "  # Loop over the iterations\n",
    "  for i in range(max_iter):\n",
    "    # Predict the labels on the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate the loss on the test data\n",
    "    loss = loss_function(y_test, y_pred)\n",
    "\n",
    "    # Append the loss to the list\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Print the loss every 10 iterations\n",
    "    if (i + 1) % 10 == 0:\n",
    "      print(f'Iteration {i + 1}, Loss: {loss}')\n",
    "\n",
    "    # Calculate the gradient of the loss function with respect to the SVM parameters\n",
    "    grad_w, grad_b = gradient_function(clf, X_train, y_train)\n",
    "\n",
    "    # Update the SVM parameters using the gradient descent formula\n",
    "    clf.coef_ -= learning_rate * grad_w\n",
    "    clf.intercept_ -= learning_rate * grad_b\n",
    "\n",
    "  # Return the final classifier and the list of losses\n",
    "  return clf, losses\n",
    "\n",
    "### Call the gradient descent function with some parameters\n",
    "clf, losses = gradient_descent(X_train, y_train, X_test, y_test, learning_rate=0.01, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### Plot the losses over iterations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(losses) + 1), losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Gradient Descent for SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.c - Selecting a Learning Rate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.d - Selecting a Regulization Technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Step 2.e - Selecting a Validation Strategy\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
