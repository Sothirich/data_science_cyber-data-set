{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "## Importing libraries\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### For testing only\n",
    "additional_path = 'drive/MyDrive/'\n",
    "\n",
    "### reading data\n",
    "df1=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back.csv')\n",
    "df2=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_BufferOverflow.csv')\n",
    "df3=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_FTPWrite.csv')\n",
    "df4=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_GuessPassword.csv')\n",
    "df5=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_NMap.csv')\n",
    "df6=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_Neptune.csv')\n",
    "df7=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_Normal.csv')\n",
    "df8=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_PortSweep.csv')\n",
    "df9=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_RootKit.csv')\n",
    "df10=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_Satan.csv')\n",
    "df11=pd.read_csv(additional_path + 'Data/Data_of_Attack_Back_Smurf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### add missing header to FTPWrite.csv\n",
    "header = df1.columns\n",
    "df3.columns = header\n",
    "\n",
    "### add new column attack\n",
    "df1['attack']=['Back']*df1.shape[0]\n",
    "df2['attack']=['BufferOverflow']*df2.shape[0]\n",
    "df3['attack']=['FTPWrite']*df3.shape[0]\n",
    "df4['attack']=['GuessPassword']*df4.shape[0]\n",
    "df5['attack']=['NMap']*df5.shape[0]\n",
    "df6['attack']=['Neptune']*df6.shape[0]\n",
    "df7['attack']=['Normal']*df7.shape[0]\n",
    "df8['attack']=['PortSweep']*df8.shape[0]\n",
    "df9['attack']=['RootKit']*df9.shape[0]\n",
    "df10['attack']=['Satan']*df10.shape[0]\n",
    "df11['attack']=['Smurf']*df11.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### concate all the data to one file\n",
    "df=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11])\n",
    "df.reset_index()\n",
    "\n",
    "# ### export data to csv file\n",
    "# df.to_csv('Data/Data_of_Attack_Appends.csv', index=False)\n",
    "\n",
    "# ### export to a variable\n",
    "# df = pd.read_csv('Data/Data_of_Attack_Appends.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### clean Unnamed columns\n",
    "df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### check for null value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### dropping rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### check duplicated row\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### drop duplicated row \n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### check data types\n",
    "# df.dtypes\n",
    "# df.columns\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "### remove space in header\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "\n",
    "### exporting cleaned data to csv file\n",
    "df.to_csv(additional_path + 'Data/Data_of_Attack_Appends_Clean.csv', index=False)\n",
    "\n",
    "### exporting cleaned data to a variable\n",
    "df = pd.read_csv(additional_path + 'Data/Data_of_Attack_Appends_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danet\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Importing libraries\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "df['protocol_type'] = le.fit_transform(df['protocol_type'])\n",
    "df['service'] = le.fit_transform(df['service'])\n",
    "df['flag'] = le.fit_transform(df['flag'])\n",
    "\n",
    "### Display the encoded categorical variables\n",
    "# df['protocol_type']\n",
    "# df['service']\n",
    "# df['flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### Normalize numerical variables\n",
    "scaler = MinMaxScaler()\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### Feature engineering\n",
    "# Create a new feature that measures the ratio of source bytes to destination bytes\n",
    "df['src_dst_ratio'] = df['src_bytes'] / (df['dst_bytes'] + 0.001)\n",
    "df['src_dst_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### Feature selection\n",
    "## Using Algorithmic Feature Selection\n",
    "# Use SelectKBest with ANOVA F-value to select the 10 best features\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selector.fit(df.drop('attack', axis=1), df['attack'])\n",
    "selected_cols = df.drop('attack', axis=1).columns[selector.get_support()]\n",
    "\n",
    "### Convertion to list\n",
    "selected_cols = selected_cols.tolist()\n",
    "\n",
    "### Adding more features to the list\n",
    "selected_cols.append('duration')\n",
    "selected_cols.append('dst_bytes')\n",
    "\n",
    "print('The selected features are:', selected_cols)\n",
    "\n",
    "## Using paper reference\n",
    "# features = ['duration', 'src_bytes', 'dst_bytes', 'count', 'srv_count', 'same_srv_rate', 'diff_srv_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_serror_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "## Importing libraries\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitou\n",
    "\n",
    "### Split the dataset into training and testing sets\n",
    "\n",
    "### Define the selected features\n",
    "selected_features = selected_cols\n",
    "\n",
    "### Split the dataset into X and y, using the selected features and the attack column\n",
    "X = df[selected_features]\n",
    "y = df['attack']\n",
    "\n",
    "### Split the dataset into training and testing sets, using 70% for training and 30% for testing, and a random state of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pitou\n",
    "\n",
    "# ### Split the dataset into training and testing sets\n",
    "# '''  \n",
    "#   We use the train_test_split function to split the dataset into \n",
    "#     features (df.drop('attack', axis=1)) and \n",
    "#     labels (df['attack']) subsets, \n",
    "#     with \n",
    "#       a test size of 0.3 (30% of the data) and \n",
    "#       a random state of 42 (for reproducibility).\n",
    "# '''\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df.drop('attack', axis=1), df['attack'], test_size=0.3, random_state=42)\n",
    "\n",
    "# ### Print the shapes of the subsets\n",
    "# print('X_train shape:', X_train.shape)\n",
    "# print('X_test shape:', X_test.shape)\n",
    "# print('y_train shape:', y_train.shape)\n",
    "# print('y_test shape:', y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
