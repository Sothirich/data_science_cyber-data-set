# Binomial Classification - Attack vs Normal

## [✅] Accuracy
This is the proportion of correct predictions among the total number of predictions. It is a simple and intuitive metric, but it can be misleading if the data is imbalanced, meaning that one class is much more frequent than the other.

## [✅] Precision
This is the proportion of correct positive predictions among the total number of positive predictions. It measures how reliable your model is when it predicts an attack. A high precision means that your model rarely misclassifies a normal activity as an attack.

## [✅] Recall
This is the proportion of correct positive predictions among the total number of actual positives. It measures how well your model can detect all the attacks. A high recall means that your model rarely misses an attack.

## [✅] F1-score
This is the harmonic mean of precision and recall. It balances both metrics and gives a single score that reflects the overall performance of your model. A high F1-score means that your model has both high precision and high recall.

# Multinomial Classification - Normal vs Back vs BufferOverflow vs FTPWrite vs GuessPassword vs NMap vs Neptune vs PortSweep vs RootKit vs Satan vs Smurf

## [❎] Confusion matrix
This is a table that shows the distribution of predictions and actual values for each class. It helps you to identify which classes are most confused by your model and where the errors occur. You can use the sklearn.metrics.confusion_matrix function to generate a confusion matrix from your y_test and y_pred arrays.

## [✅] Classification report
This is a summary of the precision, recall, and F1-score for each class, as well as the macro-average and weighted-average of these metrics across all classes. It gives you a comprehensive view of how your model performs for each class and overall. You can use the sklearn.metrics.classification_report function to generate a classification report from your y_test and y_pred arrays.